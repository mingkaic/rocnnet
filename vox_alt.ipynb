{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigation into vocabulary expansion the Learning Model (LM) described in https://arxiv.org/pdf/1806.10306.pdf [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lang.corporal import make_corpus, stringify_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_encode(indices, vocab_size):\n",
    "    encoding = []\n",
    "    for i in indices:\n",
    "        enc = np.zeros((vocab_size))\n",
    "        enc[i] = 1\n",
    "        encoding.append(enc)\n",
    "    return np.array(encoding)\n",
    "\n",
    "def sample(sess, inp, prob, seed_ix, n):\n",
    "    # Initialize first word of sample ('seed') as one-hot encoded vector.\n",
    "    x = np.zeros(inp.shape())\n",
    "    x[seed_ix] = 1\n",
    "    ixes = [seed_ix]\n",
    "\n",
    "    for _ in range(n):\n",
    "        inp.assign(x.T)\n",
    "        sess.update_target([prob])\n",
    "        p = prob.get() \n",
    "        p /= p.sum() # normalize\n",
    "\n",
    "        # Choose next char according to the distribution\n",
    "        ix = np.random.choice(range(p.shape[0]), p=p.ravel())\n",
    "        x = np.zeros(inp.shape())\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "\n",
    "    return ixes\n",
    "\n",
    "def encoded_loss(encoded_expect, encoded_result):\n",
    "    return tc.reduce_sum(-tc.log(tc.reduce_sum(encoded_result * encoded_expect, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tc.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_replace(txt, src_set, target):\n",
    "    for c in src_set:\n",
    "        txt = txt.replace(c, target)\n",
    "    return txt\n",
    "    \n",
    "f = open('books.txt', 'r')\n",
    "txt = f.read()\n",
    "txt = txt.lower()\n",
    "txt = multi_replace(txt, '.?,!()', ' \\sep ')\n",
    "txt = multi_replace(txt, ':;', ' \\col ')\n",
    "txt = multi_replace(txt, '-', ' \\dsh ')\n",
    "txt = txt.\\\n",
    "    replace('i\\'m', 'i am').\\\n",
    "    replace('can\\'t', 'can not').\\\n",
    "    replace('n\\'t', ' not').\\\n",
    "    replace('\\'re', ' are').\\\n",
    "    replace('\\'ve', ' have').\\\n",
    "    replace('\\'ll', ' will').\\\n",
    "    replace('\\'d', ' would').\\\n",
    "    replace('\\'s', '')\n",
    "txt = multi_replace(txt, '\"', '')\n",
    "words = txt.split()\n",
    "delims = ['\\sep', '\\col', '\\dsh']\n",
    "vocab = set(words)\n",
    "vocab = [v for v in vocab if len(v) > 0 and (v.isalpha() or v in delims)]\n",
    "data = []\n",
    "for word in words:\n",
    "    if word in vocab and (len(data) == 0 or word != data[-1]):\n",
    "        data.append(data)\n",
    "print(len(vocab))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, order, vocab = make_corpus('lang/grammar.json', 'lang/vocabulary3.txt', 25, 10000)\n",
    "# flatten data\n",
    "data = [e for line in data for e in line]\n",
    "data_size, vocab_size = len(data), len(order)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "gru = tc.layer.gru(vocab_size, vocab_size, seq_length,\n",
    "    weight_init=tc.unif_xavier_init(1),\n",
    "    bias_init=tc.zero_init())\n",
    "dense = tc.layer.dense([vocab_size], [vocab_size],\n",
    "    weight_init=tc.unif_xavier_init(1),\n",
    "    bias_init=tc.zero_init())\n",
    "model = tc.layer.link([\n",
    "    gru,\n",
    "    dense,\n",
    "    tc.layer.bind(lambda x: tc.softmax(x, 0, 1)),\n",
    "])\n",
    "\n",
    "data_gru = gru.get_storage()[0].get()\n",
    "data_dense = dense.get_storage()[0].get()\n",
    "\n",
    "sample_invar = tc.EVariable([1, vocab_size], 0)\n",
    "sample_prob = tc.slice(model.connect(sample_invar), 0, 1, 1)\n",
    "\n",
    "invar = tc.EVariable([seq_length, vocab_size], 0)\n",
    "exout = tc.EVariable([seq_length, vocab_size], 0)\n",
    "\n",
    "train_err = tc.sgd_train(model, invar, exout,\n",
    "    update=tc.get_adagrad(learning_rate=learning_rate, epsilon=1e-8),\n",
    "    err_func=encoded_loss)\n",
    "sess.track([sample_prob, train_err])\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length\n",
    "p = 0\n",
    "print_interval = 100\n",
    "for i in range(15000):\n",
    "    # Reset memory if appropriate\n",
    "    if p + seq_length + 1 >= len(data) or i == 0:\n",
    "        p = 0\n",
    "\n",
    "    # Get input and target sequence\n",
    "    inputs = [ix for ix in data[p:p+seq_length]]\n",
    "    encoded_inp = one_encode(inputs, vocab_size)\n",
    "    encoded_out = one_encode([ix for ix in data[p+1:p+seq_length+1]], vocab_size)\n",
    "\n",
    "    # Occasionally sample from oldModel and print result\n",
    "    if i % print_interval == 0:\n",
    "        sample_ix = sample(sess, sample_invar, sample_prob, inputs[0], 1000)\n",
    "        print('----\\n%s\\n----' % (' '.join(order[ix] for ix in sample_ix)))\n",
    "\n",
    "    # Get gradients for current oldModel based on input and target sequences\n",
    "    invar.assign(encoded_inp)\n",
    "    exout.assign(encoded_out)\n",
    "    sess.update_target([train_err])\n",
    "    loss = train_err.get()\n",
    "\n",
    "    if i % print_interval == 0:\n",
    "        print('iter %d, loss: %f, smooth loss: %f' % (i, loss, smooth_loss))\n",
    "\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    i += 1\n",
    "    \n",
    "first_sample = 'data/sample1.onnx'\n",
    "if tc.save_layers_file(first_sample, [model]):\n",
    "    print('successfully saved to {}'.format(first_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
