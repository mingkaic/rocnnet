{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook explores adding more words to the embedding vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import tenncor as tc\n",
    "from extenncor.embed import Embedding, make_embedding, vdistance\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"natural language processing and machine learning is fun and exciting\"\n",
    "text2 = \"machine language is not natural\"\n",
    "\n",
    "corpus = [[word.lower() for word in text.split()]]\n",
    "corpus_addition = 'not'\n",
    "corpus2 = [[word.lower() for word in text2.split()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(embedding, corpus):\n",
    "    training_data = []\n",
    "    # Cycle through each sentence in corpus\n",
    "    for sentence in corpus:\n",
    "        sent_len = len(sentence)\n",
    "        # Cycle through each word in sentence\n",
    "        for i in range(len(sentence)):\n",
    "            # Convert target word to one-hot\n",
    "            w_target = embedding.onehot(sentence[i])\n",
    "            # Cycle through context window\n",
    "            w_context = []\n",
    "            # Note: window_size 2 will have range of 5 values\n",
    "            for j in range(i - window, i + window+1):\n",
    "                # Criteria for context word\n",
    "                # 1. Target word cannot be context word (j != i)\n",
    "                # 2. Index must be greater or equal than 0 (j >= 0) - if not list index out of range\n",
    "                # 3. Index must be less or equal than length of sentence (j <= sent_len-1) - if not list index out of range\n",
    "                if j != i and j <= sent_len-1 and j >= 0:\n",
    "                    # Append the one-hot representation of word to w_context\n",
    "                    w_context.append(embedding.onehot(sentence[j]))\n",
    "                    # print(sentence[i], sentence[j])\n",
    "                    # training_data contains a one-hot representation of the target word and context words\n",
    "            training_data.append([w_target, w_context])\n",
    "    return np.array(training_data)\n",
    "\n",
    "def vec_sim(embedding, word, top_n):\n",
    "    v1 = embedding.get_vec(word)\n",
    "    word_sim = {}\n",
    "\n",
    "    for i in range(len(embedding)):\n",
    "        v2 = embedding[i]\n",
    "        # Find the similary score for each word in vocab\n",
    "        word = embedding.idx2word[i]\n",
    "        word_sim[word] = vdistance(v1, v2)\n",
    "\n",
    "    words_sorted = sorted(word_sim.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    for word, sim in words_sorted[:top_n]:\n",
    "        print(word, sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 34.94494700431824\n",
      "Epoch: 1 Loss: 34.394609332084656\n",
      "Epoch: 2 Loss: 34.04316842556\n",
      "Epoch: 3 Loss: 33.72573661804199\n",
      "Epoch: 4 Loss: 33.43827557563782\n",
      "Epoch: 5 Loss: 33.17703139781952\n",
      "Epoch: 6 Loss: 32.9386340379715\n",
      "Epoch: 7 Loss: 32.72014570236206\n",
      "Epoch: 8 Loss: 32.51902973651886\n",
      "Epoch: 9 Loss: 32.333131194114685\n",
      "Epoch: 10 Loss: 32.16061580181122\n",
      "Epoch: 11 Loss: 31.999934434890747\n",
      "Epoch: 12 Loss: 31.849769115447998\n",
      "Epoch: 13 Loss: 31.708996891975403\n",
      "Epoch: 14 Loss: 31.576662063598633\n",
      "Epoch: 15 Loss: 31.45194125175476\n",
      "Epoch: 16 Loss: 31.334125995635986\n",
      "Epoch: 17 Loss: 31.222601771354675\n",
      "Epoch: 18 Loss: 31.1168292760849\n",
      "Epoch: 19 Loss: 31.016337037086487\n",
      "Epoch: 20 Loss: 30.92071032524109\n",
      "Epoch: 21 Loss: 30.829578757286072\n",
      "Epoch: 22 Loss: 30.742612957954407\n",
      "Epoch: 23 Loss: 30.65951669216156\n",
      "Epoch: 24 Loss: 30.58002507686615\n",
      "Epoch: 25 Loss: 30.503898859024048\n",
      "Epoch: 26 Loss: 30.43091869354248\n",
      "Epoch: 27 Loss: 30.360889077186584\n",
      "Epoch: 28 Loss: 30.29362404346466\n",
      "Epoch: 29 Loss: 30.22896134853363\n",
      "Epoch: 30 Loss: 30.16674792766571\n",
      "Epoch: 31 Loss: 30.106844067573547\n",
      "Epoch: 32 Loss: 30.0491201877594\n",
      "Epoch: 33 Loss: 29.99345552921295\n",
      "Epoch: 34 Loss: 29.939741134643555\n",
      "Epoch: 35 Loss: 29.88787305355072\n",
      "Epoch: 36 Loss: 29.837756156921387\n",
      "Epoch: 37 Loss: 29.789303064346313\n",
      "Epoch: 38 Loss: 29.742427706718445\n",
      "Epoch: 39 Loss: 29.69705593585968\n",
      "Epoch: 40 Loss: 29.65311551094055\n",
      "Epoch: 41 Loss: 29.610536336898804\n",
      "Epoch: 42 Loss: 29.569257020950317\n",
      "Epoch: 43 Loss: 29.529216051101685\n",
      "Epoch: 44 Loss: 29.490358471870422\n",
      "Epoch: 45 Loss: 29.45263135433197\n",
      "Epoch: 46 Loss: 29.415984988212585\n",
      "Epoch: 47 Loss: 29.380372166633606\n",
      "Epoch: 48 Loss: 29.345749258995056\n",
      "Epoch: 49 Loss: 29.312073945999146\n",
      "[-0.42575166 -0.24539575  0.28960332 -0.81587696  0.27500275  0.29652333\n",
      " -0.54270196 -0.72514606 -0.22661425 -0.15576968]\n",
      "machine 1.0\n",
      "is 0.42168823\n",
      "learning 0.31845725\n"
     ]
    }
   ],
   "source": [
    "n = 10 # dimensions of word embeddings, also refer to size of hidden layer\n",
    "lr = 0.01 # learning rate\n",
    "epochs = 50 # number of training epochs\n",
    "window = 2 # context window +- center word\n",
    "\n",
    "# training\n",
    "# Initialising weight matrices\n",
    "# Both s1 and s2 should be randomly initialised but for this demo, we pre-determine the arrays (getW1 and getW2)\n",
    "# getW1 - shape (9x10) and getW2 - shape (10x9)\n",
    "\n",
    "# Find unique word counts using dictonary\n",
    "word_counts = defaultdict(int)\n",
    "for row in corpus:\n",
    "    for word in row:\n",
    "        word_counts[word] += 1\n",
    "# Generate Lookup Dictionaries (vocab)\n",
    "index_word = list(word_counts.keys())\n",
    "nwords = len(index_word)\n",
    "\n",
    "embedding = make_embedding(index_word, n)\n",
    "trainmodel = tc.api.layer.link([\n",
    "    embedding.embedding,\n",
    "    embedding.exbedding,\n",
    "    tc.api.layer.bind(tc.api.softmax),\n",
    "])\n",
    "\n",
    "training_data = generate_training_data(embedding, corpus)\n",
    "\n",
    "winput = tc.variable(np.random.rand(nwords) * 2 - 1, 'input')\n",
    "woutput = tc.variable(np.random.rand(2 * window, nwords) * 2 - 1, 'output')\n",
    "\n",
    "y_pred = trainmodel.connect(winput)\n",
    "\n",
    "train_err = tc.apply_update([trainmodel],\n",
    "    lambda error, leaves: tc.api.approx.sgd(error, leaves, lr),\n",
    "    lambda models: tc.api.reduce_sum(tc.api.pow(tc.api.extend(models[0].connect(winput), [1, 2 * window]) - woutput, 2.)))\n",
    "\n",
    "tc.optimize(\"cfg/optimizations.json\")\n",
    "\n",
    "# Cycle through each epoch\n",
    "for i in range(epochs):\n",
    "    # Intialise loss to 0\n",
    "    loss = 0\n",
    "\n",
    "    # Cycle through each training sample\n",
    "    # w_t = vector for target word, w_c = vectors for context words\n",
    "    for w_t, w_c in training_data:\n",
    "        wcdata = np.array(w_c)\n",
    "        ydata = y_pred.get().reshape(1, nwords)\n",
    "        for j in range(2 * window - wcdata.shape[0]):\n",
    "            wcdata = np.concatenate((wcdata, ydata), 0)\n",
    "        winput.assign(np.array(w_t))\n",
    "        woutput.assign(wcdata)\n",
    "        loss += train_err.get()\n",
    "    print('Epoch:', i, \"Loss:\", loss)\n",
    "\n",
    "print(embedding.get_vec(\"machine\"))\n",
    "\n",
    "# Find similar words\n",
    "vec_sim(embedding, \"machine\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = embedding.weight\n",
    "w2 = embedding.exbedding.get_storage()[0]\n",
    "\n",
    "forward = w1.get()\n",
    "backward = w2.get()\n",
    "\n",
    "mod_forward = np.concatenate((forward, np.random.uniform(-1, 1, size=(1, 10))), axis=0)\n",
    "mod_backward = np.concatenate((backward, np.random.uniform(-1, 1, size=(10, 1))), axis=1)\n",
    "\n",
    "mod_w1 = tc.variable(mod_forward, 'forward')\n",
    "mod_w2 = tc.variable(mod_backward, 'backward')\n",
    "mod_embedding = Embedding(mod_w1, mod_w2, index_word + [corpus_addition])\n",
    "mod_model = tc.api.layer.link([\n",
    "    mod_embedding.embedding,\n",
    "    mod_embedding.exbedding,\n",
    "    tc.api.layer.bind(tc.api.softmax),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 14.606863260269165\n",
      "Epoch: 1 Loss: 14.715889692306519\n",
      "Epoch: 2 Loss: 14.64060640335083\n",
      "Epoch: 3 Loss: 14.567920207977295\n",
      "Epoch: 4 Loss: 14.497643232345581\n",
      "Epoch: 5 Loss: 14.429615259170532\n",
      "Epoch: 6 Loss: 14.363690376281738\n",
      "Epoch: 7 Loss: 14.299741506576538\n",
      "Epoch: 8 Loss: 14.237656354904175\n",
      "Epoch: 9 Loss: 14.17733645439148\n",
      "Epoch: 10 Loss: 14.118694067001343\n",
      "Epoch: 11 Loss: 14.061651229858398\n",
      "Epoch: 12 Loss: 14.006137132644653\n",
      "Epoch: 13 Loss: 13.9520925283432\n",
      "Epoch: 14 Loss: 13.899460911750793\n",
      "Epoch: 15 Loss: 13.848192572593689\n",
      "Epoch: 16 Loss: 13.79824185371399\n",
      "Epoch: 17 Loss: 13.749569177627563\n",
      "Epoch: 18 Loss: 13.702137112617493\n",
      "Epoch: 19 Loss: 13.655909538269043\n",
      "Epoch: 20 Loss: 13.610855102539062\n",
      "Epoch: 21 Loss: 13.566941976547241\n",
      "Epoch: 22 Loss: 13.524141669273376\n",
      "Epoch: 23 Loss: 13.482425332069397\n",
      "Epoch: 24 Loss: 13.441765904426575\n",
      "Epoch: 25 Loss: 13.402134418487549\n",
      "Epoch: 26 Loss: 13.363505601882935\n",
      "Epoch: 27 Loss: 13.325851678848267\n",
      "Epoch: 28 Loss: 13.28914475440979\n",
      "Epoch: 29 Loss: 13.253358125686646\n",
      "Epoch: 30 Loss: 13.218464016914368\n",
      "Epoch: 31 Loss: 13.184435606002808\n",
      "Epoch: 32 Loss: 13.151244163513184\n",
      "Epoch: 33 Loss: 13.118863344192505\n",
      "Epoch: 34 Loss: 13.087262153625488\n",
      "Epoch: 35 Loss: 13.056416034698486\n",
      "Epoch: 36 Loss: 13.026294946670532\n",
      "Epoch: 37 Loss: 12.996872186660767\n",
      "Epoch: 38 Loss: 12.968120098114014\n",
      "Epoch: 39 Loss: 12.940011501312256\n",
      "Epoch: 40 Loss: 12.912520051002502\n",
      "Epoch: 41 Loss: 12.885619521141052\n",
      "Epoch: 42 Loss: 12.859285354614258\n",
      "Epoch: 43 Loss: 12.833491086959839\n",
      "Epoch: 44 Loss: 12.808213949203491\n",
      "Epoch: 45 Loss: 12.783429384231567\n",
      "Epoch: 46 Loss: 12.759116649627686\n",
      "Epoch: 47 Loss: 12.735253095626831\n",
      "Epoch: 48 Loss: 12.711817741394043\n",
      "Epoch: 49 Loss: 12.688791990280151\n",
      "[-0.3815377  -0.18165791  0.39231488 -0.6933524   0.17350991  0.20174779\n",
      " -0.4725519  -0.8487582  -0.03256607 -0.18448962]\n",
      "machine 0.99999994\n",
      "is 0.5150473\n",
      "learning 0.40191552\n"
     ]
    }
   ],
   "source": [
    "mod_winput = tc.variable(np.random.rand(nwords + 1) * 2 - 1, 'input')\n",
    "mod_woutput = tc.variable(np.random.rand(2 * window, nwords + 1) * 2 - 1, 'output')\n",
    "\n",
    "mod_y_pred = mod_model.connect(mod_winput)\n",
    "\n",
    "mod_train_err = tc.apply_update([mod_model],\n",
    "    lambda error, leaves: tc.api.approx.sgd(error, leaves, lr),\n",
    "    lambda models: tc.api.reduce_sum(tc.api.pow(tc.api.extend(models[0].connect(mod_winput), [1, 2 * window]) - mod_woutput, 2.)))\n",
    "\n",
    "tc.optimize(\"cfg/optimizations.json\")\n",
    "\n",
    "training_data2 = generate_training_data(mod_embedding, corpus2)\n",
    "\n",
    "# Cycle through each epoch\n",
    "for i in range(epochs):\n",
    "    # Intialise loss to 0\n",
    "    loss = 0\n",
    "\n",
    "    # Cycle through each training sample\n",
    "    # w_t = vector for target word, w_c = vectors for context words\n",
    "    for w_t, w_c in training_data2:\n",
    "        wcdata = np.array(w_c)\n",
    "        ydata = mod_y_pred.get().reshape(1, nwords + 1)\n",
    "        for j in range(2 * window - wcdata.shape[0]):\n",
    "            wcdata = np.concatenate((wcdata, ydata), 0)\n",
    "        mod_winput.assign(np.array(w_t))\n",
    "        mod_woutput.assign(wcdata)\n",
    "        loss += mod_train_err.get()\n",
    "    print('Epoch:', i, \"Loss:\", loss)\n",
    "\n",
    "print(mod_embedding.get_vec(\"machine\"))\n",
    "\n",
    "# Find similar words\n",
    "vec_sim(mod_embedding, \"machine\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'exciting', 'not']\n",
      "[[ 0.03082157  0.43199688  0.19140545  0.12391488 -0.1743178   0.27198088\n",
      "  -0.13611728  0.6726653   0.94228876 -0.28644916]\n",
      " [ 0.49185762 -0.06332493 -0.02737583  0.8859195  -0.7707699  -0.7410697\n",
      "  -0.8444996   0.9937508   0.6157334   0.49587414]\n",
      " [ 0.89807427  0.6270576  -0.13817546  0.5954936  -0.87662816  0.18292807\n",
      "  -0.4780913   0.9074295   0.03989711 -0.10391264]\n",
      " [-0.6278146   0.56902015 -0.09472717  0.10327913 -0.84771305  0.24537711\n",
      "   0.02896766  0.05620021  0.7442206   0.23441094]\n",
      " [-0.42575166 -0.24539575  0.28960332 -0.81587696  0.27500275  0.29652333\n",
      "  -0.54270196 -0.72514606 -0.22661425 -0.15576968]\n",
      " [ 0.0539834  -0.06859595  0.94048643 -0.88526845 -0.5705192  -0.7514896\n",
      "   0.37114364 -0.39261967 -0.20158623 -0.3559844 ]\n",
      " [-0.7845716  -0.577518    0.3956456  -0.7371265  -0.45208016 -0.2550295\n",
      "   0.7442549  -0.80364853  0.4561647  -0.63619083]\n",
      " [ 0.9400159  -0.1209984   0.925048    0.16957778  0.44497162 -0.94618815\n",
      "  -0.46468857 -0.74338514 -0.4088823  -0.60823333]\n",
      " [-0.37520775 -0.09580829 -0.8727726   0.3324664   0.14701311 -0.5135597\n",
      "   0.16694249 -0.61873037  0.13353291  0.88904524]]\n",
      "[[-0.07222415  0.30676806 -0.76516396  0.57970935 -0.4451834  -0.70028156\n",
      "   0.12170223 -1.0585781   0.5177772 ]\n",
      " [-0.86033964  0.48701963 -0.46896565  0.29564855  0.9523618  -0.5338383\n",
      "   0.18357725  0.13469766  0.08805314]\n",
      " [-0.6417193   0.6909895  -0.07646696  0.7486097   0.4089839  -0.2691874\n",
      "   0.8025657  -0.20680515  0.658405  ]\n",
      " [ 0.41195545  0.9009803   0.3929875   0.42601714 -0.00345756  0.7813889\n",
      "   0.04098479 -0.21062504  0.28464946]\n",
      " [-0.8620016  -0.43088168  0.2625412  -0.31293783  0.07251921 -0.21647261\n",
      "  -0.5721849  -0.49288872  0.19535473]\n",
      " [ 0.19101234  0.3792605   0.25838873  0.06611485 -0.14806065  0.82958317\n",
      "  -0.09742259 -0.23052423  0.73928225]\n",
      " [ 0.3641366   0.36517715 -0.91243565  0.76771045  0.49449652  1.0075294\n",
      "  -0.59016997  0.83003396 -0.48058188]\n",
      " [ 0.50674117 -0.49003583  0.6938796   0.4239342   0.18297753 -0.24560885\n",
      "  -1.0286062   0.25399545 -0.115017  ]\n",
      " [ 0.28649527  0.85633886  1.0231954   0.49835962 -0.88637805 -0.21236944\n",
      "   0.33384869 -0.5672123   0.09594509]\n",
      " [-0.7280108  -0.48189497 -0.8993099   0.5682482  -0.57402414 -0.45535588\n",
      "   0.588856    0.41388762 -0.83206666]]\n",
      "[[ 0.07431316  0.4617637   0.2052986   0.03945402 -0.23030199  0.26477453\n",
      "  -0.1471      0.56285167  0.76727146 -0.1505354 ]\n",
      " [ 0.48770657  0.00236326  0.02350849  0.7881504  -0.7805924  -0.74253005\n",
      "  -0.97860134  0.8164765   0.46158537  0.618499  ]\n",
      " [ 0.89807427  0.6270576  -0.13817546  0.5954936  -0.87662816  0.18292807\n",
      "  -0.4780913   0.9074295   0.03989711 -0.10391264]\n",
      " [-0.6278146   0.56902015 -0.09472717  0.10327913 -0.84771305  0.24537711\n",
      "   0.02896766  0.05620021  0.7442206   0.23441094]\n",
      " [-0.3815377  -0.18165791  0.39231488 -0.6933524   0.17350991  0.20174779\n",
      "  -0.4725519  -0.8487582  -0.03256607 -0.18448962]\n",
      " [ 0.0539834  -0.06859595  0.94048643 -0.88526845 -0.5705192  -0.7514896\n",
      "   0.37114364 -0.39261967 -0.20158623 -0.3559844 ]\n",
      " [-0.66492045 -0.5088231   0.43720818 -0.7396376  -0.47519714 -0.28612104\n",
      "   0.7064396  -0.78330654  0.4882404  -0.7371163 ]\n",
      " [ 0.9400159  -0.1209984   0.925048    0.16957778  0.44497162 -0.94618815\n",
      "  -0.46468857 -0.74338514 -0.4088823  -0.60823333]\n",
      " [-0.37520775 -0.09580829 -0.8727726   0.3324664   0.14701311 -0.5135597\n",
      "   0.16694249 -0.61873037  0.13353291  0.88904524]\n",
      " [ 0.929452    0.28233013 -0.6315649   0.98980325  0.28536955 -0.5586441\n",
      "  -0.5113425  -0.04108088 -0.7811497  -0.5372393 ]]\n",
      "[[-0.0615367   0.2959804  -0.80158144  0.5321827  -0.54452854 -0.6323039\n",
      "   0.21861789 -1.0037425   0.4375844  -0.10677468]\n",
      " [-0.8634101   0.4412323  -0.5079396   0.28468898  0.90170616 -0.47802445\n",
      "   0.22999173  0.17535573  0.06744235 -0.2244886 ]\n",
      " [-0.6638189   0.68561774 -0.09821029  0.7441426   0.48257616 -0.3047145\n",
      "   0.7963794  -0.23648211  0.71425927 -0.07722329]\n",
      " [ 0.40827787  0.8627057   0.32668507  0.34487352 -0.09385314  0.8521059\n",
      "   0.14948232 -0.15190342  0.21514218 -0.3333778 ]\n",
      " [-0.8284819  -0.39962712  0.34495315 -0.23398499 -0.01172751 -0.17823629\n",
      "  -0.6126108  -0.46607307  0.14249119  0.05991399]\n",
      " [ 0.16054273  0.32938644  0.29386702  0.12897797 -0.14152366  0.85800785\n",
      "  -0.15754038 -0.2165063   0.79069996  0.66943556]\n",
      " [ 0.3992411   0.34964222 -0.85391283  0.8458339   0.56239355  0.93735594\n",
      "  -0.78609866  0.7776873  -0.418842   -0.7323404 ]\n",
      " [ 0.41753238 -0.63826877  0.5790355   0.32459575  0.17227454 -0.15890361\n",
      "  -0.9324757   0.3173568  -0.07039935  0.20627062]\n",
      " [ 0.22805874  0.7674183   0.89116055  0.42311203 -0.7584769  -0.24425486\n",
      "   0.37480655 -0.59386665  0.20752332 -0.60312176]\n",
      " [-0.8540294  -0.5869937  -0.8910811   0.5302481  -0.5647541  -0.38746655\n",
      "   0.60231185  0.4591046  -0.7578945   0.484603  ]]\n",
      "cross similiarities\n",
      "natural 0.98638844\n",
      "language 0.98845017\n",
      "processing 1.0\n",
      "and 1.0\n",
      "machine 0.9731282\n",
      "learning 0.99999994\n",
      "is 0.9952815\n",
      "fun 0.9999999\n",
      "exciting 1.0\n"
     ]
    }
   ],
   "source": [
    "print(index_word + [corpus_addition])\n",
    "print(forward)\n",
    "print(backward)\n",
    "print(mod_w1.get())\n",
    "print(mod_w2.get())\n",
    "\n",
    "print('cross similiarities')\n",
    "orig_data = w1.get()\n",
    "mod_data = mod_w1.get()\n",
    "for i in range(nwords):\n",
    "    v1 = orig_data[i]\n",
    "    v2 = mod_data[i]\n",
    "    word = index_word[i]\n",
    "    print(word, vdistance(v1, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
